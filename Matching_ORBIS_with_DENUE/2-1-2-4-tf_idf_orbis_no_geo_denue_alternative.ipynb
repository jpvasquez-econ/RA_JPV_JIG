{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer 2-3 ngrams Cosine Similarity\n",
    "## ORBIS no geographical zone information available matching vs all DENUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "This notebook mainly executes the following for the firm names in `denue_final` and `orbis_final` that haven't got geographical zone information available: \n",
    "\n",
    "- Extracting the company names. \n",
    "- Training the algorithm. \n",
    "- Extracting the results. \n",
    "- Labeling the results. \n",
    "- Exporting them to a Comma Separated Values file. \n",
    "\n",
    "After this, we've got the possible name matches in `denue_final` for each company in `orbis_final`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files\n",
    "1. **orbis_final:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'` This file contains a data set where each row represents a firm with one of their names associated, also, entity, municipality and ORBIS's BVDID number.\n",
    "2. **denue_final_alternative:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'` This file contains a dataset where each row represents a firm with one of their names associated, also, the number of workers in that firm, entity, municipality and DENUE's key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'\n",
    "denue_final_alternative_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files\n",
    "1. **final_output:** `'/scratch/public/jpvasquez/MNCs_informality/Final_data/output/2-1-2-4-matches_tf_idf_orbis_no_geo_denue_alternative.csv'` This file contains all possible matches from no geo ORBIS vs all DENUE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = '/scratch/public/jpvasquez/MNCs_informality/Final_data/output/2-1-2-4-matches_tf_idf_orbis_no_geo_denue_alternative.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "These are the needed packages to run this code. In case, the machine you're running this in doesn't have any of these packages, run this code: \n",
    "\n",
    "`!pip install package_name`\n",
    "\n",
    "- **Pandas** is the package which handles importing, wrangling, cleaning and doing everything with the data. \n",
    "- **Numpy** is needed in order to declare missing values. \n",
    "- **Glob** gets all the files from a directory with a prefix. \n",
    "- **Sklearn** is a package for machile learning, we'll use the module for Natural Language Processing. \n",
    "- **Scipy** is used for scientific computing, in our case, `csr_matrix` is a dependency of `awesome_cossim_topn`. \n",
    "- **Sparse_dot_topn** performs sparse matrix multiplication followed by top-n multiplication result selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import awesome_cossim_topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final = pd.read_csv(orbis_final_file)\n",
    "denue_final = pd.read_csv(denue_final_alternative_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring options\n",
    "For customizability's sake, we'll group all the variables, options and arguments we could wish to change in the future. We'll make it in one cell, but feel free to split it into how many cells you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orbis_final\n",
    "base_data = orbis_final\n",
    "category1 = 'entidad'\n",
    "category2 = 'municipio'\n",
    "# matching_df\n",
    "matching_data = denue_final\n",
    "base_names_variable = 'companyname'\n",
    "matching_names_variable = 'firm'\n",
    "# extracting the results\n",
    "## top results\n",
    "top_results_n = 100 # number of results of top quality\n",
    "top_results_lower_bound = 0.95 # lowest score accepted for top quality matches\n",
    "## uncertain results\n",
    "uncertain_results_n = 300 # number of results of uncertain quality\n",
    "uncertain_results_lower_bound = 0.75 # lowest score accepted for uncertain quality matches\n",
    "uncertain_results_upper_bound = 0.95 # highest score accepted for uncertain quality matches\n",
    "n_uncertain_results = 5 # top n results for uncertain matches\n",
    "cores = 55 # subject to the number specified in the batch options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the data\n",
    "Drop the firms that have both categories specified and without a firm name registered. Finally, drop its duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = (base_data[(base_data.isna()[category1]) \n",
    "                     | (base_data.isna()[category2])] # keep observations that lack one of the specified categories\n",
    "           .dropna(subset = [base_names_variable]) # drop observations without variable name (firm name)\n",
    "           .drop_duplicates(ignore_index = True)) # drop possible duplicates\n",
    "matching_df = (matching_data\n",
    "               .dropna(subset = [matching_names_variable]) # drop observations without variable name (firm name)\n",
    "               .drop_duplicates(ignore_index = True)) # drop possible duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching algorithm\n",
    "The main sections are: \n",
    " \n",
    "- Get the company names. \n",
    "- Train the algorithm. \n",
    "- Extract the results. \n",
    "    - Top results. \n",
    "    - Uncertain results. \n",
    "- Save the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "###Getting variable (company) names###\n",
    "######################################\n",
    "base_names = base_df[base_names_variable] # get the list of variable names (not necessarily unique) from base_df\n",
    "matching_names = matching_df[matching_names_variable] # get the list of variable names (not necessarily unique) from base_df\n",
    "names = base_names.append(matching_names, ignore_index = True) # concatenating both lists\n",
    "\n",
    "############################\n",
    "###Training the algorithm###\n",
    "############################\n",
    "vectorizer = TfidfVectorizer(min_df = 1, # call the function, at least one item, \n",
    "                             ngram_range = (2, 3), # 2-3 ngrams, \n",
    "                             analyzer = 'char') # by characters/letters\n",
    "tf_idf_matrix = vectorizer.fit(names) # train the models with all the company names from ORBIS and DENUE\n",
    "tf_idf_matrix_base = tf_idf_matrix.transform(base_names) # transform each observation into a vector\n",
    "                                                        # and append them into a matrix\n",
    "tf_idf_matrix_matching = tf_idf_matrix.transform(matching_names) # according to the ngrams\n",
    "############################\n",
    "###Extracting the results###\n",
    "############################\n",
    "    \n",
    "############\n",
    "#Top results\n",
    "############\n",
    "possible_matches = awesome_cossim_topn(tf_idf_matrix_base, # sparse matrix multiplication, base_df matrix\n",
    "                                       tf_idf_matrix_matching.transpose(), # multiplied by the matching_df matrix\n",
    "                                       top_results_n, top_results_lower_bound, use_threads = True, # options\n",
    "                                       n_jobs = cores)\n",
    "\n",
    "possible_matches_base_df_index = possible_matches.nonzero()[0] # positions where the matches are located in base_df\n",
    "possible_matches_matching_df_index = possible_matches.nonzero()[1] # positions where the matches are located in matching_df\n",
    "    \n",
    "certain_matches = (base_df.iloc[possible_matches_base_df_index] # create certain_matches df, merge the firm names of left side\n",
    "                   .reset_index(drop = True) # select the observations by location, get index to 0, 1, ..., n\n",
    "                   .merge(matching_df.iloc[possible_matches_matching_df_index] # select the observations by location\n",
    "                          .reset_index(drop = True), left_index = True, # get index to 0, 1, ..., n\n",
    "                          right_index = True)) # merge by index, 0 with 0, 1 with 1, ...\n",
    "\n",
    "certain_matches['score'] = possible_matches.data # assign score to each match\n",
    "certain_matches['accuracy'] = 'top' # tag them as top results\n",
    "\n",
    "##################\n",
    "#Uncertain results\n",
    "##################\n",
    "    \n",
    "possible_matches = awesome_cossim_topn(tf_idf_matrix_base, # sparse matrix multiplication, base_df matrix\n",
    "                                       tf_idf_matrix_matching.transpose(), # multiplied by the matching_df matrix\n",
    "                                       uncertain_results_n, uncertain_results_lower_bound, use_threads = True, # options\n",
    "                                       n_jobs = cores)\n",
    "    \n",
    "possible_matches_base_df_index = possible_matches.nonzero()[0] # positions where the matches are located in base_df\n",
    "possible_matches_matching_df_index = possible_matches.nonzero()[1] # positions where the matches are located in matching_df\n",
    "    \n",
    "uncertain_matches = (base_df.iloc[possible_matches_base_df_index] # create certain_matches df, merge the firm names of left side\n",
    "                     .reset_index(drop = True) # select the observations by location, get index to 0, 1, ..., n\n",
    "                     .merge(matching_df.iloc[possible_matches_matching_df_index] # select the observations by location\n",
    "                                .reset_index(drop = True), left_index = True, # get index to 0, 1, ..., n\n",
    "                            right_index = True)) # merge by index, 0 with 0, 1 with 1, ...\n",
    "uncertain_matches['score'] = possible_matches.data # assign score to each match\n",
    "uncertain_matches = (uncertain_matches[uncertain_matches['score'] < uncertain_results_upper_bound] # select the matches below upper bound\n",
    "                     .sort_values(['score'], ascending = False) # sort values descending\n",
    "                     .groupby('bvdidnumber').head(n_uncertain_results)) # group by bvdidnumber, then get the n best matches\n",
    "uncertain_matches['accuracy'] = 'uncertain' # tag them as uncertain results\n",
    "\n",
    "########################\n",
    "###Saving the matches###\n",
    "########################\n",
    "matches = certain_matches.append(uncertain_matches, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate and save results\n",
    "We label the results by algorithm and DENUE's geographical selection, finally, we drop duplicate matches and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches['algorithm'] = 'tf-idf' # label the algorithm\n",
    "matches['selection'] = 'no_geo_alternative' # label the database selection\n",
    "matches.drop_duplicates(ignore_index = True).to_csv(final_output, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
