{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing DENUE's dataset for matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "\n",
    "VERSION 1.0\n",
    "Original release\n",
    "\n",
    "VERSION 1.1\n",
    "This is a modification required to process `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'` rather than the original file. The latter can be process with this file removing some cells and the hashes (#). \n",
    "\n",
    "The objective of this code is divided in these main objectives: \n",
    "\n",
    "- Recode the geographical zones. \n",
    "- Remove the firms that lack geographical location (this happened in the dataset construction of the Raw data). \n",
    "- Filter the firms which meet the following criteria: \n",
    "    - The firm has at least one business name associated (razon social). \n",
    "    - The firm is a big company (more than 50 employees). \n",
    "- Reshape the dataset such that each observation corresponds to a single possible name of a firm in its location. \n",
    "- Remove 'stopwords'. \n",
    "- Save the dataset. \n",
    "\n",
    "Note that we aren't cleaning the data because this was done in the data set's creation code in `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/code/build.do'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files\n",
    "1. **denue:** `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'` This file has geographical location variables, number of workers, ORBIS and DENUE's firm keys, generic firm names and business names (razon social). \n",
    "2. **denue_geo_corrected:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/denue_municipalities_corrected.csv'` This file contains the geographical zones of firms with their proper coding, which procedure is documented in in `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/cleaning_datasets_orbis_denue.md'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_file = '/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'\n",
    "denue_geo_corrected_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/denue_municipalities_corrected.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files\n",
    "1. **denue_final:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final.csv'` This file contains a dataset where each row represents a firm with one of their names associated, also, the number of workers in that firm, entity, municipality and DENUE's key. \n",
    "2. **denue_final_names:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_names.csv'` This file contains a data set with each unique `llave_denue` with all their names associated to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_final = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final.csv'\n",
    "denue_names_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_names.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "These are the needed packages to run this code. In case, the machine you're running this in doesn't have any of these packages, run this code: \n",
    "\n",
    "`!pip install package_name`\n",
    "\n",
    "**Pandas** is the package which handles importing, wrangling, cleaning and doing everything with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = pd.read_csv(denue_file, engine = 'python')\n",
    "denue_geo_corrected = pd.read_csv(denue_geo_corrected_file, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recode geographical zones\n",
    "Using merge on `entidad` and `municipalidad`, the correct names of the geographical zones are associated according to the assigned codes `entidad_corrected` and `municipality_corrected` in `denue_geo_corrected`. \n",
    "\n",
    "# Removing the firms that lack geographical zone\n",
    "When merging the data sets, the option *inner* is used such that the firms that remain in `denue` are the ones that have an `entidad` value in `denue` which is in the set of possible values of `entidad` in `denue_geo_corrected`. In case the data set is fixed, either *inner* or *left* can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.merge(denue_geo_corrected, how = 'inner', \n",
    "                    left_on = ['entidad', 'municipio'], \n",
    "                    right_on = ['entidad', 'municipio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the geographical information which isn't needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denue = denue.drop(columns = ['entidad', 'municipio', \n",
    "#                              'postal_code', 'localidad', \n",
    "#                              'ageb', 'manzana', \n",
    "#                              'latitud', 'longitud'])\n",
    "denue = denue.drop(columns = ['entidad', 'municipio', \n",
    "                              'postal_code', 'localidad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the firms\n",
    "## The firm has at least one business name associated (razon social). \n",
    "A dummy variable is created in order to match the corresponding condition. If the firm has one of their business names not equal to a missing value, then the firm is elegible of type 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/linux/anaconda3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "denue['elegible_1'] = 0\n",
    "denue.loc[(denue['razon_social_1'].notna()) | \n",
    "          (denue['razon_social_2'].notna()) | \n",
    "          (denue['razon_social_3'].notna()) | \n",
    "          (denue['razon_social_4'].notna()) | \n",
    "          (denue['razon_social_5'].notna()) | \n",
    "          (denue['razon_social_6'].notna()) | \n",
    "          (denue['razon_social_7'].notna()) | \n",
    "          (denue['razon_social_8'].notna())]['elegible_1'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The firm is a big company (more than 50 employees). \n",
    "A dummy variable is created in order to match the corresponding condition. If the firm has 51 to 100 employees, 101 to 250 or 251 or more, then the firm is elegible of type 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue['elegible_2'] = denue['n_workers'].map({'1) 0 a 5 personas': 0, # write 1 in you want a specific category\n",
    "                                              '2) 6 a 10 personas': 0,  # in the dataset, else, write 0\n",
    "                                              '3) 11 a 30 personas': 0, \n",
    "                                              '4) 31 a 50 personas': 0, \n",
    "                                              '5) 51 a 100 personas': 1, \n",
    "                                              '6) 101 a 250 personas': 1, \n",
    "                                              '7) 251 y más personas': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the observations \n",
    "We select the observations that meet the criteria, also, we reset the index to keep it aligned with the number of rows, the latter procedure we'll be done multiple times. Finally, we drop the `elegible` variables. Remember, these are dummy variables we created, so, you can write `True` or `False`, `1` or `0` to include or exclude them, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = (denue[(denue['elegible_1'] == 1) | (denue['elegible_2'] == 1)]\n",
    "         .reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for a reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the unnecessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.drop(columns = 'elegible_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.rename(columns = {'name_1': 'firm1', \n",
    "                                'name_2': 'firm2', \n",
    "                                'name_3': 'firm3', \n",
    "                                'name_4': 'firm4', \n",
    "                                'name_5': 'firm5', \n",
    "                                'name_6': 'firm6', \n",
    "                                'name_7': 'firm7', \n",
    "                                'razon_social_1': 'firm8', \n",
    "                                'razon_social_2': 'firm9', \n",
    "                                'razon_social_3': 'firm10', \n",
    "                                'razon_social_4': 'firm11', \n",
    "                                'razon_social_5': 'firm12', \n",
    "                                'razon_social_6': 'firm13', \n",
    "                                'razon_social_7': 'firm14', \n",
    "                                'name_8': 'firm15', \n",
    "                                'razon_social_8': 'firm16', \n",
    "                                'entidad_corrected': 'entidad', \n",
    "                                'municipio_corrected': 'municipio'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop duplicates from data set C\n",
    "Now that we've got two data sets appended, there might be some duplicates with the same *llave_denue*. So, we keep the ones in data set d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = (denue.sort_values(by = 'data_base', ascending = False)\n",
    "         .drop_duplicates(subset = ['llave_denue'], \n",
    "                          keep = 'first')\n",
    "         .drop(columns = ['data_base']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data set\n",
    "The data set is reshaped in order that every row represents a single possible name of a single firm with their respective location. Also, note that not every `name` or `razon_social` variable has necessarily a name in it, there can be just one name reported. This will give missing values or NAs in our dataset, so we drop the missing values. Finally, we reset the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue = (pd.wide_to_long(denue, stubnames = 'firm', i = 'index', j='n')\n",
    "         .dropna()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing _ with spaces\n",
    "First, we remove everything that's not a letter or a white space f in every `firm`. Then, removing the _ can leave multiple spaces, so we make sure that between every word there's only one space with `strip` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue['firm'] = (denue['firm'].str.replace('[^\\w\\s]','')\n",
    "                 .str.replace('_',' ')\n",
    "                 .str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the unique names for each `llave_denue`\n",
    "We select the key column `llave_denue` and the firm names `firm`, then, we drop possible duplicates and reset the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue_names = (denue[['llave_denue', 'firm']]\n",
    "               .copy()\n",
    "               .drop_duplicates()\n",
    "               .reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy column to count unique firm names by `llave_denue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names['n'] = (denue_names\n",
    "                    .groupby(['llave_denue'])\n",
    "                    .cumcount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo reshaping from long to wide\n",
    "This is done in order to get an observation with a unique `llave_denue` with all the possible `firm` names associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue_names = denue_names.set_index(['llave_denue', 'n']).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite MultiIndex with desired column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names.columns = ['firm1', \n",
    "                       'firm2', \n",
    "                       'firm3', \n",
    "                       'firm4', \n",
    "                       'firm5', \n",
    "                       'firm6', \n",
    "                       'firm7', \n",
    "                       'firm8', \n",
    "                       'firm9', \n",
    "                       'firm10', \n",
    "                       'firm11']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save `denue_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names.to_csv(denue_names_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "There are multiple words that don't add more information or quality to our matching algorithms. We can assure this because all the firms have the same location: México, and we don't care about the company's structure in name similarity. Also, by looking manually in the data set, we detected common words that could qualify as stopwords and made a list with them. Then, for each possible firm name, we create a vector without the stopwords listed and joined them with spaces again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['de', 'a', 's', 'l', \n",
    "                'r', 'sade', 'v', 'c', \n",
    "                'b', 'sa', 'cv', 'sab', \n",
    "                'mexicana', 'mexicano', 'limitada', 'rl', \n",
    "                'mexico', 'latinoamerica', 'srl', 'mejico', \n",
    "                'via', 'its', 'funds', 'y', \n",
    "                'sapi', 'enr', 'sofom', 'mxico', \n",
    "                'latin', 'america', 'internacional', 'mexicanos', \n",
    "                'mexicanas', 'mex', 'er']\n",
    "denue['firm'] = (denue['firm'].apply(lambda x: ' '\n",
    "                                     .join([word for word in x.split() \n",
    "                                            if word not in (remove_words)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure there aren't duplicates\n",
    "We drop the duplicates and reset the index, notice that there are many of them after remove punctuation, accents and multiple spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue = (denue.drop_duplicates()\n",
    "         .reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>municipio</th>\n",
       "      <th>n_workers</th>\n",
       "      <th>elegible_2</th>\n",
       "      <th>llave_denue</th>\n",
       "      <th>entidad</th>\n",
       "      <th>firm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toluca</td>\n",
       "      <td>7) 251 y más personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2421131</td>\n",
       "      <td>mexico</td>\n",
       "      <td>abarrotes del centro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahome</td>\n",
       "      <td>5) 51 a 100 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3776371</td>\n",
       "      <td>sinaloa</td>\n",
       "      <td>secretaria agricultura ganade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ahome</td>\n",
       "      <td>5) 51 a 100 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3778355</td>\n",
       "      <td>sinaloa</td>\n",
       "      <td>iglesia jesucristo los san</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ahome</td>\n",
       "      <td>5) 51 a 100 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3778309</td>\n",
       "      <td>sinaloa</td>\n",
       "      <td>iglesia apostolica la fe en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahome</td>\n",
       "      <td>6) 101 a 250 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3777973</td>\n",
       "      <td>sinaloa</td>\n",
       "      <td>japama planta ingeniero jose her</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334984</th>\n",
       "      <td>centro</td>\n",
       "      <td>6) 101 a 250 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6399538</td>\n",
       "      <td>tabasco</td>\n",
       "      <td>fideicomisof1596 fideicomisof159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334985</th>\n",
       "      <td>paracho</td>\n",
       "      <td>5) 51 a 100 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2566782</td>\n",
       "      <td>michoacan_de_ocampo</td>\n",
       "      <td>preciliano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334986</th>\n",
       "      <td>cuauhtemoc</td>\n",
       "      <td>6) 101 a 250 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6313867</td>\n",
       "      <td>ciudad_de_mexico</td>\n",
       "      <td>alfaomega grupo editorial d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334987</th>\n",
       "      <td>cuauhtemoc</td>\n",
       "      <td>5) 51 a 100 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1005620</td>\n",
       "      <td>ciudad_de_mexico</td>\n",
       "      <td>caf restaurantes del centro d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334988</th>\n",
       "      <td>saltillo</td>\n",
       "      <td>6) 101 a 250 personas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6300064</td>\n",
       "      <td>coahuila_de_zaragoza</td>\n",
       "      <td>fernando cerecedo flores</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334989 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         municipio              n_workers  elegible_2 llave_denue  \\\n",
       "0           toluca  7) 251 y más personas         1.0     2421131   \n",
       "1            ahome   5) 51 a 100 personas         1.0     3776371   \n",
       "2            ahome   5) 51 a 100 personas         1.0     3778355   \n",
       "3            ahome   5) 51 a 100 personas         1.0     3778309   \n",
       "4            ahome  6) 101 a 250 personas         1.0     3777973   \n",
       "...            ...                    ...         ...         ...   \n",
       "334984      centro  6) 101 a 250 personas         1.0     6399538   \n",
       "334985     paracho   5) 51 a 100 personas         1.0     2566782   \n",
       "334986  cuauhtemoc  6) 101 a 250 personas         1.0     6313867   \n",
       "334987  cuauhtemoc   5) 51 a 100 personas         1.0     1005620   \n",
       "334988    saltillo  6) 101 a 250 personas         1.0     6300064   \n",
       "\n",
       "                     entidad                              firm  \n",
       "0                     mexico              abarrotes del centro  \n",
       "1                    sinaloa     secretaria agricultura ganade  \n",
       "2                    sinaloa        iglesia jesucristo los san  \n",
       "3                    sinaloa       iglesia apostolica la fe en  \n",
       "4                    sinaloa  japama planta ingeniero jose her  \n",
       "...                      ...                               ...  \n",
       "334984               tabasco  fideicomisof1596 fideicomisof159  \n",
       "334985   michoacan_de_ocampo                        preciliano  \n",
       "334986      ciudad_de_mexico       alfaomega grupo editorial d  \n",
       "334987      ciudad_de_mexico     caf restaurantes del centro d  \n",
       "334988  coahuila_de_zaragoza          fernando cerecedo flores  \n",
       "\n",
       "[334989 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data set\n",
    "We save the data set to a Comma Separated Values file and we order the columns in our preferred order by naming them one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue[['llave_denue', 'n_workers','entidad', 'municipio', 'firm', 'elegible_2']].to_csv(denue_final, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
