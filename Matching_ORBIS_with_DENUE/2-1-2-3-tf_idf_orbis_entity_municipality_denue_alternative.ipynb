{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer 2-3 ngrams Cosine Similarity\n",
    "## Controlling ORBIS by entity and municipality and entire DENUE's data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "This notebook mainly executes the following for the firm names in `denue_final` and `orbis_final`: \n",
    "\n",
    "- Filtering the desired geographical zones in both data sets. \n",
    "- Extracting the company names. \n",
    "- Training the algorithm. \n",
    "- Extracting the results. \n",
    "- Labeling the results. \n",
    "- Exporting them to a Comma Separated Values file. \n",
    "\n",
    "After this, we've got the possible name matches in `denue_final` for each company in `orbis_final`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files\n",
    "1. **orbis_final:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'` This file contains a data set where each row represents a firm with one of their names associated, also, entity, municipality and ORBIS's BVDID number.\n",
    "2. **denue_final_alternative:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'` This file contains a dataset where each row represents a firm with one of their names associated, also, the number of workers in that firm, entity, municipality and DENUE's key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'\n",
    "denue_final_alternative_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files\n",
    "1. **output_file_prefix:** `'/scratch/public/jpvasquez/MNCs_informality/Final_data/output/orbis_entity_municipality_denue/denue_alternative/td-idf/matches_tf_idf_orbis_'` These files contain the possible matches of each municipality.\n",
    "2. **final_output:** `'/scratch/public/jpvasquez/MNCs_informality/Final_data/output/matches_tf_idf_orbis_entity_municipality_denue_alternative.csv'` This file is a concatenation of all the `output_file_prefix` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_prefix = '/scratch/public/jpvasquez/MNCs_informality/Final_data/output/orbis_entity_municipality_denue/denue_alternative/td-idf/matches_tf_idf_orbis_'\n",
    "final_output = '/scratch/public/jpvasquez/MNCs_informality/Final_data/output/matches_tf_idf_orbis_entity_municipality_denue_alternative.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "These are the needed packages to run this code. In case, the machine you're running this in doesn't have any of these packages, run this code: \n",
    "\n",
    "`!pip install package_name`\n",
    "\n",
    "- **Pandas** is the package which handles importing, wrangling, cleaning and doing everything with the data. \n",
    "- **Numpy** is needed in order to declare missing values. \n",
    "- **Glob** gets all the files from a directory with a prefix. \n",
    "- **Sklearn** is a package for machile learning, we'll use the module for Natural Language Processing. \n",
    "- **Scipy** is used for scientific computing, in our case, `csr_matrix` is a dependency of `awesome_cossim_topn`. \n",
    "- **Sparse_dot_topn** performs sparse matrix multiplication followed by top-n multiplication result selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import awesome_cossim_topn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final = pd.read_csv(orbis_final_file)\n",
    "denue_final = pd.read_csv(denue_final_alternative_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring options\n",
    "For customizability's sake, we'll group all the variables, options and arguments we could wish to change in the future. We'll make it in one cell, but feel free to split it into how many cells you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_df\n",
    "base_data = orbis_final\n",
    "category1 = 'entidad' # control 1 for groupby\n",
    "category2 = 'municipio' # control 2 for groupby\n",
    "# matching_df\n",
    "matching_data = denue_final\n",
    "base_names_variable = 'companyname' # control 1 for groupby\n",
    "matching_names_variable = 'firm' # control 2 for groupby\n",
    "# extracting the results\n",
    "## top results\n",
    "top_results_n = 100 # number of results of top quality\n",
    "top_results_lower_bound = 0.95 # lowest score accepted for top quality matches\n",
    "## uncertain results\n",
    "uncertain_results_n = 300 # number of results of top quality\n",
    "uncertain_results_lower_bound = 0.75 # lowest score accepted for uncertain quality matches\n",
    "uncertain_results_upper_bound = 0.95 # highest score accepted for uncertain quality matches\n",
    "n_uncertain_results = 5 # top n results for uncertain matches\n",
    "cores = 55 # subject to the number specified in the batch options\n",
    "##################################\n",
    "### don't change anything below###\n",
    "##################################\n",
    "categories = [category1, category2]\n",
    "matching_df = (matching_data\n",
    "                   .copy()\n",
    "                   .dropna(subset = [matching_names_variable]) # drop observations without variable name (firm name)\n",
    "                   .drop_duplicates(ignore_index = True)) # drop possible duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching algorithm\n",
    "Since this is a loop, we can't divide the algorithm in separate cells, so, we'll comment it with #. \n",
    "The main sections are: \n",
    "\n",
    "- Filter the datasets with their corresponding categories. \n",
    "- Get the company names. \n",
    "- Train the algorithm. \n",
    "- Extract the results. \n",
    "    - Top results. \n",
    "    - Uncertain results. \n",
    "- Save the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for base_group, base_df in base_data.groupby(by = categories): \n",
    "    i += 1\n",
    "    print(f'Matching {base_group}, combination {i} out of 398')\n",
    "    #########################\n",
    "    ###Filter the datasets###\n",
    "    #########################\n",
    "    base_df = (base_df.copy()\n",
    "               .dropna(subset = [base_names_variable]) # drop observations without variable name (firm name)\n",
    "               .drop_duplicates(ignore_index=True)) # drop possible duplicates\n",
    "    ######################################\n",
    "    ###Getting variable (company) names###\n",
    "    ######################################\n",
    "    base_names = base_df[base_names_variable] # get the list of variable names (not necessarily unique) from base_df\n",
    "    matching_names = matching_df[matching_names_variable] # get the list of variable names \n",
    "                                                          # (not necessarily unique) from matching_df\n",
    "    names = base_names.append(matching_names, ignore_index=True) # concatenating both lists\n",
    "    ############################\n",
    "    ###Training the algorithm###\n",
    "    ############################\n",
    "    vectorizer = TfidfVectorizer(min_df = 1, ngram_range = (2, 3), analyzer = 'char') # call the function \n",
    "                                                                                      # at least one item, 2-3 ngrams, \n",
    "                                                                                      # by characters/letters\n",
    "    tf_idf_matrix = vectorizer.fit(names) # train the models with all the company names from ORBIS and DENUE\n",
    "    tf_idf_matrix_base = tf_idf_matrix.transform(base_names) # transform each observation into a vector \n",
    "                                                             # and append them into a matrix\n",
    "    tf_idf_matrix_matching = tf_idf_matrix.transform(matching_names) # according to the ngrams\n",
    "    ############################\n",
    "    ###Extracting the results###\n",
    "    ############################\n",
    "    \n",
    "    ############\n",
    "    #Top results\n",
    "    ############\n",
    "    \n",
    "    possible_matches = awesome_cossim_topn(tf_idf_matrix_base, # sparse matrix multiplication, base_df matrix\n",
    "                                           tf_idf_matrix_matching.transpose(), # multiplied by the matching_df matrix\n",
    "                                           top_results_n, top_results_lower_bound, use_threads = True, # options\n",
    "                                           n_jobs = cores)\n",
    "    \n",
    "    possible_matches_base_df_index = possible_matches.nonzero()[0] # positions where the matches are located in base_df\n",
    "    possible_matches_matching_df_index = possible_matches.nonzero()[1] # positions where the matches are located in matching_df\n",
    "    \n",
    "    certain_matches = (base_df.iloc[possible_matches_base_df_index] # create certain_matches df, merge the firm names of left side\n",
    "                       .reset_index(drop = True) # select the observations by location, get index to 0, 1, ..., n\n",
    "                       .merge(matching_df.iloc[possible_matches_matching_df_index] # select the observations by location\n",
    "                              .reset_index(drop = True), left_index = True, # get index to 0, 1, ..., n\n",
    "                              right_index = True)) # merge by index, 0 with 0, 1 with 1, ...\n",
    "    certain_matches['score'] = possible_matches.data # assign score to each match\n",
    "    certain_matches['accuracy'] = 'top' # tag them as top results\n",
    "    \n",
    "    ##################\n",
    "    #Uncertain results\n",
    "    ##################\n",
    "    \n",
    "    possible_matches = awesome_cossim_topn(tf_idf_matrix_base, # sparse matrix multiplication, base_df matrix\n",
    "                                           tf_idf_matrix_matching.transpose(), # multiplied by the matching_df matrix\n",
    "                                           uncertain_results_n, uncertain_results_lower_bound, use_threads = True, # options\n",
    "                                           n_jobs = cores)\n",
    "    \n",
    "    possible_matches_base_df_index = possible_matches.nonzero()[0] # positions where the matches are located in base_df\n",
    "    possible_matches_matching_df_index = possible_matches.nonzero()[1] # positions where the matches are located in matching_df\n",
    "    \n",
    "    uncertain_matches = (base_df.iloc[possible_matches_base_df_index] # create certain_matches df, merge the firm names of left side\n",
    "                         .reset_index(drop = True) # select the observations by location, get index to 0, 1, ..., n\n",
    "                         .merge(matching_df.iloc[possible_matches_matching_df_index] # select the observations by location\n",
    "                                .reset_index(drop = True), left_index = True, # get index to 0, 1, ..., n\n",
    "                                right_index = True)) # merge by index, 0 with 0, 1 with 1, ...\n",
    "    uncertain_matches['score'] = possible_matches.data # assign score to each match\n",
    "    uncertain_matches = (uncertain_matches[uncertain_matches['score'] < uncertain_results_upper_bound] # select the matches below upper bound\n",
    "                         .sort_values(['score'], ascending = False) # sort values descending\n",
    "                         .groupby('bvdidnumber').head(n_uncertain_results)) # group by bvdidnumber, then get the n best matches\n",
    "    uncertain_matches['accuracy'] = 'uncertain' # tag them as uncertain results\n",
    "    \n",
    "    ########################\n",
    "    ###Saving the matches###\n",
    "    ########################\n",
    "    \n",
    "    matches = certain_matches.append(uncertain_matches, ignore_index = True) # append certain with uncertain matches\n",
    "    file_name = output_file_prefix + base_group[0] + '_' + base_group[1] + '_denue_alternative' + '.csv' # create file name\n",
    "    matches.drop_duplicates(ignore_index = True).to_csv(file_name, index = False) # remove duplicates and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate and save results\n",
    "First, we concatenate all the files with the prefix `output_file_prefix`, then we label them by algorithm and DENUE's geographical selection, finally, we drop duplicate matches and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_matches = pd.concat([pd.read_csv(f) for f in glob.glob(output_file_prefix +'*.csv')], ignore_index = True)\n",
    "joint_matches['algorithm'] = 'td-idf'\n",
    "joint_matches['selection'] = 'orbis_entity_municipality_denue_alternative'\n",
    "joint_matches.drop_duplicates(ignore_index = True).to_csv(final_output, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
