{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing DENUE's dataset for matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "\n",
    "VERSION 1.0\n",
    "Original release\n",
    "\n",
    "VERSION 1.1\n",
    "This is a modification required to process `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'` rather than the original file. The latter can be process with this file removing some cells and the hashes (#). \n",
    "\n",
    "The objective of this code is divided in these main objectives: \n",
    "\n",
    "- Recode the geographical zones. \n",
    "- Remove the firms that lack geographical location (this happened in the dataset construction of the Raw data). \n",
    "- Filter the firms which meet the following criteria: \n",
    "    - The firm has at least one business name associated (razon social). \n",
    "    - The firm is a big company (more than 50 employees). \n",
    "- Reshape the dataset such that each observation corresponds to a single possible name of a firm in its location. \n",
    "- Remove 'stopwords'. \n",
    "- Save the dataset. \n",
    "\n",
    "Note that we aren't cleaning the data because this was done in the data set's creation code in `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/code/build.do'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files\n",
    "1. **denue:** `'/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'` This file has geographical location variables, number of workers, ORBIS and DENUE's firm keys, generic firm names and business names (razon social). \n",
    "2. **denue_geo_corrected:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/denue_municipalities_corrected.csv'` This file contains the geographical zones of firms with their proper coding, which procedure is documented in in `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/cleaning_datasets_orbis_denue.md'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_file = '/scratch/public/jpvasquez/MNCs_informality/Raw_data/DENUE/output/denue_version_both.csv'\n",
    "denue_geo_corrected_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/data/denue_municipalities_corrected.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files\n",
    "1. **denue_alternative:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'` This file contains a dataset where each row represents a firm with one of their names associated, also, the number of workers in that firm, entity, municipality and DENUE's key. \n",
    "2. **denue_alternative_names:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_alternative_names.csv'` This file contains a data set with each unique `llave_denue` with all their names associated to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_alternative = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final_alternative.csv'\n",
    "denue_alternative_names_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_alternative_names.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "These are the needed packages to run this code. In case, the machine you're running this in doesn't have any of these packages, run this code: \n",
    "\n",
    "`!pip install package_name`\n",
    "\n",
    "**Pandas** is the package which handles importing, wrangling, cleaning and doing everything with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = pd.read_csv(denue_file, engine = 'python')\n",
    "denue_geo_corrected = pd.read_csv(denue_geo_corrected_file, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recode geographical zones\n",
    "Using merge on `entidad` and `municipalidad`, the correct names of the geographical zones are associated according to the assigned codes `entidad_corrected` and `municipality_corrected` in `denue_geo_corrected`. \n",
    "\n",
    "# Removing the firms that lack geographical zone\n",
    "When merging the data sets, the option *inner* is used such that the firms that remain in `denue` are the ones that have an `entidad` value in `denue` which is in the set of possible values of `entidad` in `denue_geo_corrected`. In case the data set is fixed, either *inner* or *left* can be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.merge(denue_geo_corrected, how = 'inner', \n",
    "                    left_on = ['entidad', 'municipio'], \n",
    "                    right_on = ['entidad', 'municipio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the geographical information which isn't needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denue = denue.drop(columns = ['entidad', 'municipio', \n",
    "#                              'postal_code', 'localidad', \n",
    "#                              'ageb', 'manzana', \n",
    "#                              'latitud', 'longitud'])\n",
    "denue = denue.drop(columns = ['entidad', 'municipio', \n",
    "                              'postal_code', 'localidad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the firms\n",
    "## The firm has at least one business name associated (razon social). \n",
    "A dummy variable is created in order to match the corresponding condition. If the firm has one of their business names not equal to a missing value, then the firm is elegible of type 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/linux/anaconda3.7/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "denue['elegible_1'] = 0\n",
    "denue.loc[(denue['razon_social_1'].notna()) | \n",
    "          (denue['razon_social_2'].notna()) | \n",
    "          (denue['razon_social_3'].notna()) | \n",
    "          (denue['razon_social_4'].notna()) | \n",
    "          (denue['razon_social_5'].notna()) | \n",
    "          (denue['razon_social_6'].notna()) | \n",
    "          (denue['razon_social_7'].notna()) | \n",
    "          (denue['razon_social_8'].notna())]['elegible_1'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The firm is a big company (more than 50 employees). \n",
    "A dummy variable is created in order to match the corresponding condition. If the firm has 51 to 100 employees, 101 to 250 or 251 or more, then the firm is elegible of type 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue['elegible_2'] = denue['n_workers'].map({'1) 0 a 5 personas': 0, # write 1 in you want a specific category\n",
    "                                              '2) 6 a 10 personas': 1,  # in the dataset, else, write 0\n",
    "                                              '3) 11 a 30 personas': 1, \n",
    "                                              '4) 31 a 50 personas': 1, \n",
    "                                              '5) 51 a 100 personas': 1, \n",
    "                                              '6) 101 a 250 personas': 1, \n",
    "                                              '7) 251 y más personas': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the `n_workers` variable numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue['n_workers'] = denue['n_workers'].map({'1) 0 a 5 personas': 1, \n",
    "                                              '2) 6 a 10 personas': 2,  \n",
    "                                              '3) 11 a 30 personas': 3, \n",
    "                                              '4) 31 a 50 personas': 4, \n",
    "                                              '5) 51 a 100 personas': 5, \n",
    "                                              '6) 101 a 250 personas': 6, \n",
    "                                              '7) 251 y más personas': 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the observations \n",
    "We select the observations that meet the criteria, also, we reset the index to keep it aligned with the number of rows, the latter procedure we'll be done multiple times. Finally, we drop the `elegible` variables. Remember, these are dummy variables we created, so, you can write `True` or `False`, `1` or `0` to include or exclude them, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = (denue[(denue['elegible_1'] == 1) | (denue['elegible_2'] == 1)]\n",
    "         .reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data for a reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the unnecessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.drop(columns = 'elegible_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = denue.rename(columns = {'name_1': 'firm1', \n",
    "                                'name_2': 'firm2', \n",
    "                                'name_3': 'firm3', \n",
    "                                'name_4': 'firm4', \n",
    "                                'name_5': 'firm5', \n",
    "                                'name_6': 'firm6', \n",
    "                                'name_7': 'firm7', \n",
    "                                'razon_social_1': 'firm8', \n",
    "                                'razon_social_2': 'firm9', \n",
    "                                'razon_social_3': 'firm10', \n",
    "                                'razon_social_4': 'firm11', \n",
    "                                'razon_social_5': 'firm12', \n",
    "                                'razon_social_6': 'firm13', \n",
    "                                'razon_social_7': 'firm14', \n",
    "                                'name_8': 'firm15', \n",
    "                                'razon_social_8': 'firm16', \n",
    "                                'entidad_corrected': 'entidad', \n",
    "                                'municipio_corrected': 'municipio'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide dataset by their origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_c = denue.query(\"data_base == 'c'\")\n",
    "denue_d = denue.query(\"data_base == 'd'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data set\n",
    "The data set is reshaped in order that every row represents a single possible name of a single firm with their respective location. Also, note that not every `name` or `razon_social` variable has necessarily a name in it, there can be just one name reported. This will give missing values or NAs in our dataset, so we drop the missing values. Finally, we reset the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue_c = (pd\n",
    "           .wide_to_long(denue_c, \n",
    "                       stubnames = 'firm', \n",
    "                       i = 'index', \n",
    "                       j = 'n')\n",
    "           .dropna()\n",
    "           .drop(columns = ['data_base']))\n",
    "denue_d = (pd\n",
    "           .wide_to_long(denue_d, \n",
    "                       stubnames = 'firm', \n",
    "                       i = 'index', \n",
    "                       j='n')\n",
    "           .dropna()\n",
    "           .drop(columns = ['data_base']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rejoin the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue = (pd.concat([denue_c, denue_d], \n",
    "                  ignore_index = True)\n",
    "         .drop_duplicates(ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing _ with spaces\n",
    "First, we remove everything that's not a letter or a white space f in every `firm`. Then, removing the _ can leave multiple spaces, so we make sure that between every word there's only one space with `strip` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue['firm'] = (denue['firm'].str.replace('[^\\w\\s]','')\n",
    "                 .str.replace('_',' ')\n",
    "                 .str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the unique names for each `llave_denue`\n",
    "We select the key column `llave_denue` and the firm names `firm`, then, we drop possible duplicates and reset the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue_names = (denue[['llave_denue', 'firm']]\n",
    "               .copy()\n",
    "               .drop_duplicates()\n",
    "               .reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dummy column to count unique firm names by `llave_denue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names['n'] = (denue_names\n",
    "                    .groupby(['llave_denue'])\n",
    "                    .cumcount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo reshaping from long to wide\n",
    "This is done in order to get an observation with a unique `llave_denue` with all the possible `firm` names associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue_names = denue_names.set_index(['llave_denue', 'n']).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwrite MultiIndex with desired column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names.columns = ['firm1', \n",
    "                       'firm2', \n",
    "                       'firm3', \n",
    "                       'firm4', \n",
    "                       'firm5', \n",
    "                       'firm6', \n",
    "                       'firm7', \n",
    "                       'firm8', \n",
    "                       'firm9', \n",
    "                       'firm10', \n",
    "                       'firm11',\n",
    "                       'firm12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save `denue_alternative_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue_names.to_csv(denue_alternative_names_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords\n",
    "There are multiple words that don't add more information or quality to our matching algorithms. We can assure this because all the firms have the same location: México, and we don't care about the company's structure in name similarity. Also, by looking manually in the data set, we detected common words that could qualify as stopwords and made a list with them. Then, for each possible firm name, we create a vector without the stopwords listed and joined them with spaces again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['de', 'a', 's', 'l', \n",
    "                'r', 'sade', 'v', 'c', \n",
    "                'b', 'sa', 'cv', 'sab', \n",
    "                'mexicana', 'mexicano', 'limitada', 'rl', \n",
    "                'mexico', 'latinoamerica', 'srl', 'mejico', \n",
    "                'via', 'its', 'funds', 'y', \n",
    "                'sapi', 'enr', 'sofom', 'mxico', \n",
    "                'latin', 'america', 'internacional', 'mexicanos', \n",
    "                'mexicanas', 'mex', 'er']\n",
    "denue['firm'] = (denue['firm'].apply(lambda x: ' '\n",
    "                                     .join([word for word in x.split() \n",
    "                                            if word not in (remove_words)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure there aren't duplicates\n",
    "We drop the duplicates and reset the index, notice that there are many of them after remove punctuation, accents and multiple spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denue = (denue.drop_duplicates()\n",
    "         .reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_workers</th>\n",
       "      <th>municipio</th>\n",
       "      <th>llave_denue</th>\n",
       "      <th>entidad</th>\n",
       "      <th>elegible_2</th>\n",
       "      <th>firm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1000064</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gestion social atencion pens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1000200</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rinconcito alameda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1001040</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1001651</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>frenos clutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1002679</td>\n",
       "      <td>aguascalientes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>san isidro labrador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051864</th>\n",
       "      <td>2.0</td>\n",
       "      <td>tehuacan</td>\n",
       "      <td>6226801</td>\n",
       "      <td>puebla</td>\n",
       "      <td>1.0</td>\n",
       "      <td>typhoon sport coalition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051865</th>\n",
       "      <td>2.0</td>\n",
       "      <td>san_juan_del_rio</td>\n",
       "      <td>6226843</td>\n",
       "      <td>queretaro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>typhoon sports coalition p i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051866</th>\n",
       "      <td>2.0</td>\n",
       "      <td>san_juan_del_rio</td>\n",
       "      <td>6226844</td>\n",
       "      <td>queretaro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>typhoon sport coalition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051867</th>\n",
       "      <td>2.0</td>\n",
       "      <td>apetatitlan_de_antonio_carvajal</td>\n",
       "      <td>6226891</td>\n",
       "      <td>tlaxcala</td>\n",
       "      <td>1.0</td>\n",
       "      <td>typhoon sport coalition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051868</th>\n",
       "      <td>2.0</td>\n",
       "      <td>merida</td>\n",
       "      <td>6226953</td>\n",
       "      <td>yucatan</td>\n",
       "      <td>1.0</td>\n",
       "      <td>typhoon sports coalition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051869 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         n_workers                        municipio llave_denue  \\\n",
       "0              3.0                   aguascalientes     1000064   \n",
       "1              2.0                   aguascalientes     1000200   \n",
       "2              2.0                   aguascalientes     1001040   \n",
       "3              3.0                   aguascalientes     1001651   \n",
       "4              3.0                   aguascalientes     1002679   \n",
       "...            ...                              ...         ...   \n",
       "3051864        2.0                         tehuacan     6226801   \n",
       "3051865        2.0                 san_juan_del_rio     6226843   \n",
       "3051866        2.0                 san_juan_del_rio     6226844   \n",
       "3051867        2.0  apetatitlan_de_antonio_carvajal     6226891   \n",
       "3051868        2.0                           merida     6226953   \n",
       "\n",
       "                entidad  elegible_2                          firm  \n",
       "0        aguascalientes         1.0  gestion social atencion pens  \n",
       "1        aguascalientes         1.0            rinconcito alameda  \n",
       "2        aguascalientes         1.0                     cafeteria  \n",
       "3        aguascalientes         1.0                 frenos clutch  \n",
       "4        aguascalientes         1.0           san isidro labrador  \n",
       "...                 ...         ...                           ...  \n",
       "3051864          puebla         1.0       typhoon sport coalition  \n",
       "3051865       queretaro         1.0  typhoon sports coalition p i  \n",
       "3051866       queretaro         1.0       typhoon sport coalition  \n",
       "3051867        tlaxcala         1.0       typhoon sport coalition  \n",
       "3051868         yucatan         1.0      typhoon sports coalition  \n",
       "\n",
       "[3051869 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data set\n",
    "We save the data set to a Comma Separated Values file and we order the columns in our preferred order by naming them one by one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "denue[['llave_denue', 'n_workers','entidad', 'municipio', 'firm', 'elegible_2']].to_csv(denue_alternative, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
