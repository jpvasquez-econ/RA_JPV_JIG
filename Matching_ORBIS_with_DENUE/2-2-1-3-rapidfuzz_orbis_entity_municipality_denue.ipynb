{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rapid Fuzz\n",
    "## Controlling ORBIS by entity and municipality and entire DENUE's data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information\n",
    "This notebook mainly executes the following for the firm names in `denue_final` and `orbis_final`: \n",
    "\n",
    "- Filtering the desired geographical zones in both data sets. \n",
    "- Extracting the company names. \n",
    "- Training the algorithm. \n",
    "- Extracting the results. \n",
    "- Labeling the results. \n",
    "- Exporting them to a Comma Separated Values file. \n",
    "\n",
    "After this, we've got the possible name matches in `denue_final` for each company in `orbis_final`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files\n",
    "1. **orbis_final:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'` This file contains a data set where each row represents a firm with one of their names associated, also, entity, municipality and ORBIS's BVDID number.\n",
    "2. **denue_final:** `'/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final.csv'` This file contains a dataset where each row represents a firm with one of their names associated, also, the number of workers in that firm, entity, municipality and DENUE's key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/orbis_final.csv'\n",
    "denue_final_file = '/scratch/public/jpvasquez/MNCs_informality/Intermediate_data/output/denue_final.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files\n",
    "1. **output_file_prefix:** `'/scratch/public/jpvasquez/MNCs_informality/Final_data/output/orbis_entity_municipality_denue/denue/rapidfuzz/matches_tf_idf_orbis_'` These files contain the possible matches of each municipality.\n",
    "2. **final_output:** `'/scratch/public/jpvasquez/MNCs_informality/Final_data/output/matches_rapidfuzz_orbis_entity_municipality_denue.csv'` This file is a concatenation of all the `output_file_prefix` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_prefix = '/scratch/public/jpvasquez/MNCs_informality/Final_data/output/orbis_entity_municipality_denue/denue/rapidfuzz/matches_tf_idf_orbis_'\n",
    "final_output = '/scratch/public/jpvasquez/MNCs_informality/Final_data/output/matches_rapidfuzz_orbis_entity_municipality_denue.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "These are the needed packages to run this code. In case, the machine you're running this in doesn't have any of these packages, run this code: \n",
    "\n",
    "`!pip install package_name`\n",
    "\n",
    "- **Pandas** is the package which handles importing, wrangling, cleaning and doing everything with the data. \n",
    "- **Numpy** is needed in order to declare missing values. \n",
    "- **Glob** gets all the files from a directory with a prefix. \n",
    "- **Sklearn** is a package for machile learning, we'll use the module for Natural Language Processing. \n",
    "- **Scipy** is used for scientific computing, in our case, `csr_matrix` is a dependency of `awesome_cossim_topn`. \n",
    "- **Sparse_dot_topn** performs sparse matrix multiplication followed by top-n multiplication result selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/linux/anaconda3.7/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import dask as dd\n",
    "from rapidfuzz import fuzz\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_final = pd.read_csv(orbis_final_file)\n",
    "denue_final = pd.read_csv(denue_final_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaring options\n",
    "For customizability's sake, we'll group all the variables, options and arguments we could wish to change in the future. We'll make it in one cell, but feel free to split it into how many cells you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_df\n",
    "base_data = orbis_final\n",
    "category1 = 'entidad' # control 1 for groupby\n",
    "category2 = 'municipio' # control 2 for groupby\n",
    "# matching_df\n",
    "matching_data = denue_final\n",
    "base_names_variable = 'companyname' # control 1 for groupby\n",
    "matching_names_variable = 'firm' # control 2 for groupby\n",
    "# extracting the results\n",
    "## top results\n",
    "top_results_lower_bound = 0.95 # lowest score accepted for top quality matches\n",
    "## uncertain results\n",
    "uncertain_results_lower_bound = 0.75 # lowest score accepted for uncertain quality matches\n",
    "uncertain_results_upper_bound = 0.95 # highest score accepted for uncertain quality matches\n",
    "n_uncertain_results = 5 # top n results for uncertain matches\n",
    "cores = 55 # subject to the number specified in the batch options\n",
    "##################################\n",
    "### don't change anything below###\n",
    "##################################\n",
    "categories = [category1, category2]\n",
    "matching_df = (matching_data\n",
    "                   .copy()\n",
    "                   .dropna(subset = [matching_names_variable]) # drop observations without variable name (firm name)\n",
    "                   .drop_duplicates(ignore_index = True)) # drop possible duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching algorithm\n",
    "Since this is a loop, we can't divide the algorithm in separate cells, so, we'll comment it with #. \n",
    "The main sections are: \n",
    "\n",
    "- Filter the datasets with their corresponding categories. \n",
    "- Get the company names. \n",
    "- Train the algorithm. \n",
    "- Extract the results. \n",
    "    - Top results. \n",
    "    - Uncertain results. \n",
    "- Save the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for base_group, base_df in base_data.groupby(by = categories): \n",
    "    i += 1\n",
    "    print(f'Matching {base_group}, combination {i} out of 398')\n",
    "    #########################\n",
    "    ###Filter the datasets###\n",
    "    #########################\n",
    "    base_df = (base_df.copy()\n",
    "               .dropna(subset = [base_names_variable]) # drop observations without variable name (firm name)\n",
    "               .drop_duplicates(ignore_index = True)) # drop possible duplicates\n",
    "    ###################################\n",
    "    ###Converting to Dask Dataframes###\n",
    "    ###################################\n",
    "    base_df = dd.from_pandas(base_df, chunksize = 50)\n",
    "    matching_df = dd.from_pandas(matching_df, chunksize = 500)\n",
    "    ########################################\n",
    "    ###All combinations of the dataframes###\n",
    "    ########################################\n",
    "    base_df = base_df.assign(key = 0) # set key to match on\n",
    "    matching_df = matching_df.assign(key = 0) # set key to match on\n",
    "    matches = dd.merge(base_df, matching_df, suffixes=('_x', '_y'), on = \"key\", \n",
    "                       how = 'outer', shuffle = 'tasks').compute() # tasks: to use distributed computations on all nodes\n",
    "    ############################\n",
    "    ###Extracting the results###\n",
    "    ############################\n",
    "    matches['score'] = matches.apply(lambda x: \n",
    "                                     fuzz.token_sort_ratio(x[base_names_variable], \n",
    "                                                                     x[matching_names_variable])/100, \n",
    "                                     axis = 1) # apply algorithm\n",
    "    ############\n",
    "    #Top results\n",
    "    ############\n",
    "    certain_matches = matches[matches['score'] > top_results_lower_bound].copy()\n",
    "    certain_matches['accuracy'] = 'top' # tag them as top results\n",
    "    \n",
    "    ##################\n",
    "    #Uncertain results\n",
    "    ##################\n",
    "    uncertain_matches = (matches[(matches['score'] < uncertain_results_upper_bound) \n",
    "                                 & (matches['score'] > uncertain_results_lower_bound)] # select the matches below upper bound\n",
    "                         .copy() \n",
    "                         .sort_values(['score'], ascending = False) # sort values descending\n",
    "                         .groupby('bvdidnumber').head(n_uncertain_results)) # group by bvdidnumber, then get the n best matches\n",
    "    uncertain_matches['accuracy'] = 'uncertain' # tag them as uncertain results\n",
    "    \n",
    "    ########################\n",
    "    ###Saving the matches###\n",
    "    ########################\n",
    "    \n",
    "    matches = certain_matches.append(uncertain_matches, ignore_index = True) # append certain with uncertain matches\n",
    "    file_name = output_file_prefix + base_group[0] + '_' + base_group[1] + '_denue' + '.csv' # create file name\n",
    "    matches.drop_duplicates(ignore_index = True).to_csv(file_name, index = False) # remove duplicates and save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate and save results\n",
    "First, we concatenate all the files with the prefix `output_file_prefix`, then we label them by algorithm and DENUE's geographical selection, finally, we drop duplicate matches and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_matches = pd.concat([pd.read_csv(f) for f in \n",
    "                           glob.glob(output_file_prefix +'*.csv')], ignore_index=True) # concatenate the results\n",
    "joint_matches['algorithm'] = 'rapidfuzz' # label the algorithm\n",
    "joint_matches['selection'] = 'orbis_entity_municipality_denue' # label the database selection\n",
    "joint_matches.drop_duplicates(ignore_index = True).to_csv(final_output, index = False) # drop duplicates and save "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
